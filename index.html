<html>

<head>
	<script src="https://www.google.com/jsapi" type="text/javascript"></script>
	<script type="text/javascript">google.load("jquery", "1.3.2");</script>


	<style type="text/css">
		body {
			font-family: 'Noto Sans', sans-serif;
			color: #4a4a4a;
			font-size: 1em;
			font-weight: 400;
			line-height: 1.5;
			margin-left: auto;
			margin-right: auto;
			width: 1100px;
		}

		.title {
			color: #363636;
			font-size: 4rem;
			line-height: 1.125;
			font-weight: 200;
			font-family: 'Google Sans', sans-serif;
		}

		.publication-title {
			font-family: 'Google Sans', sans-serif;
		}

		h1 {
			font-size: 40px;
			font-weight: 500;
		}

		h2 {
			font-size: 35px;
			font-weight: 300;
		}

		h3 {
			font-size: 1px;
			font-weight: 300;
		}

		.subtitle,
		.title {
			word-break: break-word;
		}

		.title.is-2 {
			font-size: 4rem;
			font-weight: 400;
		}

		.title.is-3 {
			font-size: 2.5rem;
			font-weight: 400;
		}

		.content h2 {
			font-weight: 600;
			font-family: 'Noto Sans', sans-serif;
			line-height: 1.125;
		}

		.disclaimerbox {
			background-color: #eee;
			border: 1px solid #eeeeee;
			border-radius: 10px;
			-moz-border-radius: 10px;
			-webkit-border-radius: 10px;
			padding: 20px;
		}

		video.header-vid {
			height: 140px;
			border: 1px solid rgba(0, 0, 0, 0.212);
			border-radius: 10px;
			-moz-border-radius: 10px;
			-webkit-border-radius: 10px;
		}

		img.header-img {
			height: 140px;
			border: 1px solid black;
			border-radius: 10px;
			-moz-border-radius: 10px;
			-webkit-border-radius: 10px;
		}

		img.rounded {
			border: 1px solid #eeeeee;
			border-radius: 10px;
			-moz-border-radius: 10px;
			-webkit-border-radius: 10px;
		}

		a:link,
		a:visited {
			color: #2994c5;
			text-decoration: none;
		}

		a:hover {
			color: #0e889e;
		}

		td.dl-link {
			height: 160px;
			text-align: center;
			font-size: 22px;
		}

		.layered-paper-big {
			/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
			box-shadow:
				0px 0px 1px 1px rgba(0, 0, 0, 0.35),
				/* The top layer shadow */
				5px 5px 0 0px #fff,
				/* The second layer */
				5px 5px 1px 1px rgba(0, 0, 0, 0.35),
				/* The second layer shadow */
				10px 10px 0 0px #fff,
				/* The third layer */
				10px 10px 1px 1px rgba(0, 0, 0, 0.35),
				/* The third layer shadow */
				15px 15px 0 0px #fff,
				/* The fourth layer */
				15px 15px 1px 1px rgba(0, 0, 0, 0.35),
				/* The fourth layer shadow */
				20px 20px 0 0px #fff,
				/* The fifth layer */
				20px 20px 1px 1px rgba(0, 0, 0, 0.35),
				/* The fifth layer shadow */
				25px 25px 0 0px #fff,
				/* The fifth layer */
				25px 25px 1px 1px rgba(0, 0, 0, 0.35);
			/* The fifth layer shadow */
			margin-left: 10px;
			margin-right: 45px;
		}

		.paper-big {
			/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
			box-shadow:
				0px 0px 1px 1px rgba(0, 0, 0, 0.35);
			/* The top layer shadow */

			margin-left: 10px;
			margin-right: 45px;
		}

		.button.is-dark {
			background-color: #363636;
			border-color: transparent;
			color: #fff;
		}

		.button.is-rounded {
			border-radius: 290486px;
			padding-left: calc(1em + .25em);
			padding-right: calc(1em + .25em);
		}

		.button.is-normal {
			font-size: 1rem;
		}

		.button .icon,
		.button .icon.is-large,
		.button .icon.is-medium,
		.button .icon.is-small {
			height: 1.5em;
			width: 1.5em;
		}

		.icon {
			align-items: center;
			display: inline-flex;
			justify-content: center;
			height: 1.5rem;
			width: 1.5rem;
		}

		.button.is-normal {
			font-size: 1rem;
		}

		.button {
			background-color: #fff;
			border-color: #dbdbdb;
			border-width: 1px;
			color: #363636;
			cursor: pointer;
			justify-content: center;
			padding-bottom: calc(.5em - 1px);
			padding-left: 1em;
			padding-right: 1em;
			padding-top: calc(.5em - 1px);
			text-align: center;
			white-space: nowrap;
		}

		.button .icon:first-child:not(:last-child) {
			margin-left: calc(-.5em - 1px);
			margin-right: .25em;
		}

		.button.is-rounded {
			border-radius: 290486px;
			padding-left: calc(1em + .25em);
			padding-right: calc(1em + .25em);
		}

		.button.is-normal {
			font-size: 1rem;
		}

		.button.is-dark {
			background-color: #363636;
			border-color: transparent;
			color: #fff;
		}

		.link-block a {
			margin-top: 15px;
			margin-bottom: 15px;
		}

		.button {
			background-color: #fff;
			border-color: #dbdbdb;
			border-width: 1px;
			color: #363636;
			cursor: pointer;
			justify-content: center;
			padding-bottom: calc(.5em - 1px);
			padding-left: 1em;
			padding-right: 1em;
			padding-top: calc(.5em - 1px);
			text-align: center;
			white-space: nowrap;
		}

		.button,
		.file-cta,
		.file-name,
		.input,
		.pagination-ellipsis,
		.pagination-link,
		.pagination-next,
		.pagination-previous,
		.select select,
		.textarea {
			-moz-appearance: none;
			-webkit-appearance: none;
			align-items: center;
			border: 1px solid transparent;
			border-top-color: transparent;
			border-top-width: 1px;
			border-right-color: transparent;
			border-right-width: 1px;
			border-bottom-color: transparent;
			border-bottom-width: 1px;
			border-left-color: transparent;
			border-left-width: 1px;
			border-radius: 4px;
			box-shadow: none;
			display: inline-flex;
			font-size: 1rem;
			height: 2.5em;
			justify-content: flex-start;
			line-height: 1.5;
			padding-bottom: calc(.5em - 1px);
			padding-left: calc(.75em - 1px);
			padding-right: calc(.75em - 1px);
			padding-top: calc(.5em - 1px);
			position: relative;
			vertical-align: top;
		}

		.breadcrumb,
		.button,
		.delete,
		.file,
		.is-unselectable,
		.modal-close,
		.pagination-ellipsis,
		.pagination-link,
		.pagination-next,
		.pagination-previous,
		.tabs {
			-webkit-touch-callout: none;
			-webkit-user-select: none;
			-moz-user-select: none;
			-ms-user-select: none;
			user-select: none;
		}

		a {
			color: #3273dc;
			cursor: pointer;
			text-decoration: none;
		}

		a {
			color: #007bff;
			text-decoration: none;
			background-color: transparent;
		}

		*,
		::after,
		::before {
			box-sizing: inherit;
		}

		*,
		::before,
		::after {
			box-sizing: border-box;
		}

		span {
			font-style: inherit;
			font-weight: inherit;
		}

		.layered-paper {
			/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
			box-shadow:
				0px 0px 1px 1px rgba(0, 0, 0, 0.35),
				/* The top layer shadow */
				5px 5px 0 0px #fff,
				/* The second layer */
				5px 5px 1px 1px rgba(0, 0, 0, 0.35),
				/* The second layer shadow */
				10px 10px 0 0px #fff,
				/* The third layer */
				10px 10px 1px 1px rgba(0, 0, 0, 0.35);
			/* The third layer shadow */
			margin-top: 5px;
			margin-left: 10px;
			margin-right: 30px;
			margin-bottom: 5px;
		}

		.vert-cent {
			position: relative;
			top: 50%;
			transform: translateY(-50%);
		}

		hr {
			border: 0;
			height: 1px;
			background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
		}
	</style>

	<title>Revisiting Depth Completion from a Stereo Matching Perspective for Cross-domain Generalization</title>
	<meta property="og:image" content="./assets/teaser.png" />
	<!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title"
		content="Revisiting Depth Completion from a Stereo Matching Perspective for Cross-domain Generalization" />
	<meta property="og:url" content="https://vppdc.github.io/">
	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag() { dataLayer.push(arguments); }
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
	<script type="text/x-mathjax-config">
		MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
	</script>
	<script type="text/javascript"
		src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
		</script>
	<!-- <script type="text/javascript" id="MathJax-script" async
		src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
	</script> -->

</head>

<body>
	<br>
	<center>
		<h1 class="title is-1 publication-title" style="margin-bottom: 0"><strong>Revisiting Depth Completion from a
				Stereo Matching Perspective for Cross-domain Generalization</strong></h1>
		<!-- <h2 class="title is-2 publication-title" style="margin-top: 0; margin-bottom: 0">3D from multi-view and sensors
		</h2> -->
		<br>
		<h3 class="title is-3 publication-title" style="margin-top: 0; margin-bottom: 0">3DV 2024 </h3>
		<br>
		<table align=center width="1100px">
			<table align=center width="1000px">
				<tr>
					<td align=center width="180px">
						<center>
							<span style="font-size:25px"><a href="https://bartn8.github.io/">Luca
									Bartolomei</a></span>
						</center>
					</td>
					<td align=center width="150px">
						<center>
							<span style="font-size:25px"><a href="https://mattpoggi.github.io/">Matteo
									Poggi</a></span>
						</center>
					</td>
					<td align=center width="150px">
						<center>
							<span style="font-size:25px"><a href="https://andreaconti.github.io/">Andrea
									Conti</a></span>
						</center>
					</td>
					<td align=center width="150px">
						<center>
							<span style="font-size:25px"><a href="https://fabiotosi92.github.io/">Fabio
									Tosi</a></span>
						</center>
					</td>
					<td align=center width="180px">
						<center>
							<span style="font-size:25px"><a
									href="https://stefanomattoccia.github.io/">Stefano
									Mattoccia</a></span>
						</center>
					</td>
				</tr>
			</table>
			<br>
			<table align=center width="750px">
				<table align=center width="750px">
					<tr>
						<td align=center width="200px">
							<center>
								<span style="font-size:22px">University of Bologna</span>
							</center>
						</td>
					</tr>
				</table>

				<!-- PDF Link. -->
				<span class="link-block">
					<a href="assets/paper.pdf" class="external-link button is-normal is-rounded is-dark"
						style="background-color:#000000;">
						<span class="icon">
							<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false"
								data-prefix="fas" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg"
								viewBox="0 0 384 512" data-fa-i2svg="">
								<path fill="currentColor"
									d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z">
								</path>
							</svg><!-- <i class="fas fa-file-pdf"></i> Font Awesome fontawesome.com -->
						</span>
						<span>Paper</span>
					</a>
				</span>


				<span class="link-block">
					<a href="assets/paper-supp.pdf" class="external-link button is-normal is-rounded is-dark"
						style="background-color:#000000;">
						<span class="icon" disabled>
							<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false"
								data-prefix="fas" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg"
								viewBox="0 0 384 512" data-fa-i2svg="">
								<path fill="currentColor"
									d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z">
								</path>
							</svg><!-- <i class="fas fa-file-pdf"></i> Font Awesome fontawesome.com -->
						</span>
						<span>Supplement</span>
					</a>
				</span>

				<!-- Poster Link. -->
				<!-- <span class="link-block">
					<a href="assets/poster.pdf" class="external-link button is-normal is-rounded is-dark"
						style="background-color:#000000;">
						<span class="icon">
							<svg class="svg-inline--fa fa-palette fa-w-16" aria-hidden="true" focusable="false"
								data-prefix="fas" data-icon="palette" role="img" xmlns="http://www.w3.org/2000/svg"
								viewBox="0 0 512 512" data-fa-i2svg="">
								<path fill="currentColor"
									d="M204.3 5C104.9 24.4 24.8 104.3 5.2 203.4c-37 187 131.7 326.4 258.8 306.7 41.2-6.4 61.4-54.6 42.5-91.7-23.1-45.4 9.9-98.4 60.9-98.4h79.7c35.8 0 64.8-29.6 64.9-65.3C511.5 97.1 368.1-26.9 204.3 5zM96 320c-17.7 0-32-14.3-32-32s14.3-32 32-32 32 14.3 32 32-14.3 32-32 32zm32-128c-17.7 0-32-14.3-32-32s14.3-32 32-32 32 14.3 32 32-14.3 32-32 32zm128-64c-17.7 0-32-14.3-32-32s14.3-32 32-32 32 14.3 32 32-14.3 32-32 32zm128 64c-17.7 0-32-14.3-32-32s14.3-32 32-32 32 14.3 32 32-14.3 32-32 32z">
								</path>
							</svg>
						</span>
						<span>Poster</span>
					</a>
				</span> -->


				<!-- Code Link. -->
				<span class="link-block">
					<a href="https://github.com/bartn8/vppdc/" class="external-link button is-normal is-rounded is-dark"
						style="background-color:#000000;">
						<span class="icon">
							<svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false"
								data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg"
								viewBox="0 0 496 512" data-fa-i2svg="">
								<path fill="currentColor"
									d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z">
								</path>
							</svg><!-- <i class="fab fa-github"></i> Font Awesome fontawesome.com -->
						</span>
						<span>Code</span>
					</a>
				</span>
			</table>
	</center>
	<br>
	<center>
		<table align=center width="1000px">
			<tr>
				<td>
					<center>
						<img class="round" width="1000px" src="./assets/teaser.png" />
					</center>
				</td>
			</tr>
		</table>
		<table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
			<tr>
				<td>
					<p style="text-align: justify;">
						<strong>Synth-to-real generalization.</strong> Given an NYU Depth V2 frame and 500 sparse depth
						points (a), our framework with RAFT-Stereo trained only on the Sceneflow synthetic dataset (e)
						outperforms the generalization capability of state-of-the-art depth completion networks NLSPN
						(b), SpAgNet (c), and CompletionFormer (d) - all trained on the same synthetic dataset.
					</p>
				</td>
			</tr>
		</table>
	</center>

	<br>
	<br>
	<hr>

	<center>
		<h1>Abstract</h1>
	</center>

	<table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
		<tr>
			<td>
				<p style="text-align: justify;">
					<i>"This paper proposes a new framework for depth completion robust against domain-shifting issues. It exploits the generalization capability of modern stereo networks to face depth completion, by processing fictitious stereo pairs obtained through a virtual pattern projection paradigm.
						Any stereo network or traditional stereo matcher can be seamlessly plugged into our framework, allowing for the deployment of a virtual stereo setup that is future-proof against advancement in the stereo field. Exhaustive experiments on cross-domain generalization support our claims. Hence, we argue that our framework can help depth completion to reach new deployment scenarios."</i>
				</p>
			</td>
		</tr>
	</table>
	<br>

	<hr>

	<center>
		<h1>Method</h1>
	</center>
	
	<br>


	<table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
		<center>
			<tr>
				<td>
					<h2> 1 - Problems</h2>
					<ol>
						<li>
							<p style="text-align: justify;">The typical depth completion setup, consisting of a conventional camera registered with an active depth sensor, has received much attention recently due to the minimalist setup needed and the outstanding results achieved by learning-based methods to tackle this task. Despite these achievements, state-of-the-art networks struggle with out-of-domain data distribution, making their practical deployment challenging.</p>
							<table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
								<tr>
									<td><center>NYU Depth V2</center></td>
									<td><center>KITTIDC</center></td>
								</tr>
								<tr>
									<td><img src="assets/spagnet-nyu-nyu.png" width="215px" /></td>
									<td><img src="assets/spagnet-nyu-kitti.png" width="775px" /></td>
								</tr>
								<tr>
									<td><img src="assets/spagnet-nyu-nyu_errormap.png" width="215px" /></td>
									<td><img src="assets/spagnet-nyu-kitti_errormap.png" width="775px" /></td>
								</tr>
							</table>
							<table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
								<tr>
									<td>
										<p style="text-align: justify;">
											<strong>From indoor to outdoor generalization capabilities.</strong> Recent depth completion network <a href="https://github.com/andreaconti/sparsity-agnostic-depth-completion">SpAgNet</a> achieves impressive performance when dealing with in-domain scenarios (<a href="https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html">NYU Depth V2</a>). However, when tested in outdoor environment (<a href="https://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_completion">KITTIDC</a>), the network shows catastrophic behaviour.
										</p>
									</td>
								</tr>
							</table>
							<br>
							<table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
								<tr>
									<td><center>NYU Depth V2</center></td>
									<td><center>KITTIDC</center></td>
								</tr>
								<tr>
									<td><img src="assets/spagnet-kitti-nyu.png" width="215px" /></td>
									<td><img src="assets/spagnet-kitti-kitti.png" width="775px" /></td>
								</tr>
								<tr>
									<td><img src="assets/spagnet-kitti-nyu_errormap.png" width="215px" /></td>
									<td><img src="assets/spagnet-kitti-kitti_errormap.png" width="775px" /></td>
								</tr>
							</table>
							<table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
								<tr>
									<td>
										<p style="text-align: justify;">
											<strong>From outdoor to indoor generalization capabilities.</strong> We observe a similar behaviuor when we train <a href="https://github.com/andreaconti/sparsity-agnostic-depth-completion">SpAgNet</a> on <a href="https://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_completion">KITTIDC</a> and test it on <a href="https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html">NYU Depth V2</a>.
										</p>
									</td>
								</tr>
							</table>
						</li>
					</ol>
				</td>
			</tr>
			<tr>
				<td>
					<h2> 2 - Proposal</h2>
					<ol>
						<li>
							<p style="text-align: justify;">
								Inspired by the outstanding generalization capability of modern stereo networks, we cast depth completion as if it were a stereo problem through a depth-based <a href="https://vppstereo.github.io">virtual pattern projection</a> paradigm.
								In contrast to most depth completion approaches conceived to densify the input sparse depth seeds according to the image content and monocular cues only, our strategy treats depth completion as a correspondence problem through existing stereo matchers. This is achieved by processing virtual stereo pairs characterized by less domain-specific features, enabling much higher robustness to out-of-domain issues.
							</p>
							<table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
								<tr>
									<td><center>NYU Depth V2</center></td>
									<td><center>KITTIDC</center></td>
								</tr>
								<tr>
									<td><img src="assets/raftstereo-nyu-nyu.png" width="215px" /></td>
									<td><img src="assets/raftstereo-nyu-kitti.png" width="775px" /></td>
								</tr>
								<tr>
									<td><img src="assets/raftstereo-nyu-nyu_errormap.png" width="215px" /></td>
									<td><img src="assets/raftstereo-nyu-kitti_errormap.png" width="775px" /></td>
								</tr>
							</table>
							<table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
								<tr>
									<td>
										<p style="text-align: justify;">
											<strong>From indoor to outdoor generalization capabilities.</strong> Our proposal achieves similar performance w.r.t. <a href="https://github.com/andreaconti/sparsity-agnostic-depth-completion">SpAgNet</a> when dealing with in-domain scenarios (<a href="https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html">NYU Depth V2</a>). However, when tested in outdoor environment (<a href="https://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_completion">KITTIDC</a>), our framework outperforms <a href="https://github.com/andreaconti/sparsity-agnostic-depth-completion">SpAgNet</a>.
										</p>
									</td>
								</tr>
							</table>
							<br>
							<table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
								<tr>
									<td><center>NYU Depth V2</center></td>
									<td><center>KITTIDC</center></td>
								</tr>
								<tr>
									<td><img src="assets/raftstereo-kitti-nyu.png" width="215px" /></td>
									<td><img src="assets/raftstereo-kitti-kitti.png" width="775px" /></td>
								</tr>
								<tr>
									<td><img src="assets/raftstereo-kitti-nyu_errormap.png" width="215px" /></td>
									<td><img src="assets/raftstereo-kitti-kitti_errormap.png" width="775px" /></td>
								</tr>
							</table>
							<table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
								<tr>
									<td>
										<p style="text-align: justify;">
											<strong>From outdoor to indoor generalization capabilities.</strong> We observe a similar behaviuor when we train VPP4DC on <a href="https://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_completion">KITTIDC</a> and test it on <a href="https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html">NYU Depth V2</a>.
										</p>
									</td>
								</tr>
							</table>
						</li>
						<li>
							<p style="text-align: justify;">
								Extensive experimental results with multiple datasets and networks demonstrate that our proposal vastly outperforms state-of-the-art concerning generalization capability.
							</p>
						</li>
					</ol>
				</td>
			</tr>
			<tr>
				<td>
					<h2> 3 - Virtual Pattern Projection for Depth Completion (VPP4DC)</h2>
					
					<center>
						<img class="round" width="1000px" src="./assets/framework.png" />
					</center>
					<table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
						<tr>
							<td>
								<p style="text-align: justify;">
									<strong>Overview of the basic VPP4DC paradigm.</strong> 
									On the left, the proposed stereo setup we designed on top of the standard depth completion sensor suite enclosed in the green area. On the right, an outline of the proposed random projection that allows feeding a stereo matcher with a fictitious virtual patterned stereo pair and, optionally, an RGB image to tackle depth completion.
								</p>
							</td>
						</tr>
					</table>

					<p style="text-align: justify;">
						<!--This section describes our approach for robustly facing depth completion, by deploying stereo matchers that process purely hallucinated image pairs generated according to the <a href="https://vppstereo.github.io">virtual pattern projection paradigm</a> -- as if the scene were framed by two fictitious cameras and a pattern projector hitting the scene in sparse regions. The intuition behind this choice is to leverage the robustness of state-of-the-art stereo matchers at locating features across frames and domains to overcome the intrinsic limitations of conventional completion frameworks.-->
						Given the standard setup for depth completion enclosed in the green area in the previous figure -- consisting of a depth sensor (i) and an optional RGB camera (ii) -- our proposal casts the task as a stereo correspondence problem using a virtual stereo setup with two fictitious cameras, one in the same position as the actual RGB device if present (ii), and the other (iii) at a distance $b$, \ie the virtual stereo baseline. While the focal length $f$ of the virtual cameras is constrained by the depth sensor (i) or the RGB camera (ii), the virtual stereo baseline $b$ is a hyper-parameter.
						We assume that the real RGB camera and the depth sensor are calibrated and we set the origin of the reference system in the camera. Therefore, we can project sparse depth points $\mathbf{Z}$ in the reference RGB camera view using the camera matrix $\mathbf{K}_r$ and the roto-translation $[\mathbf{R}_r|\mathbf{T}_r]$ between the depth sensor and the RGB camera:
					</p>

					$$Z_r = \mathbf{K}_r \left[ \mathbf{R}_r | \mathbf{T}_r \right]  \mathbf{Z}$$

					<p style="text-align: justify;">
						where $Z_r$ is the sparse depth map projected into the reference image plane. The proximity of the depth sensor and RGB camera can reduce occlusion issues when projecting, although they cannot be entirely avoided -- yet, can be easily identified and filtered out.
						Then, we place an additional target virtual camera sharing the same intrinsics $\mathbf{K}_r$ of the other virtual device at a horizontal distance to create a virtual baseline $b$.
						Although we will stick to this setup, it is worth noting that the target virtual camera is not constrained to the horizontal axis. 
					</p>

					<p style="text-align: justify;">
						In the outlined setup, we aim to project onto the two fictitious cameras appropriate virtual patterns coherent with the 3D structure of the scene framed by the depth sensor, as if a projector were present in the setup.
						At first, the sparse depth points are converted to the disparity domain using the parameters of the virtual stereo rig as follows:
					</p>

					$$D_r = \frac{b \cdot f}{Z_r}$$

					<p style="text-align: justify;">
						where $Z_r$ is the sparse depth map aligned with the reference image, $b$ is the virtual baseline, and $f$ is the focal length of the virtual cameras (the same as the RGB camera). $D_r$ is the sparse disparity map aligned with the reference virtual image $I_r$ and the RGB image $I$. 
					</p>


					<p style="text-align: justify;">
						Given our setup and the sparse depth points converted into disparity values, we can project the same pattern onto the fictitious reference $I_r$ and target $I_t$ cameras for each point $(x,y)$ with an available disparity value $d(x,y)$ in the reference image. It can be done by recalling that with a calibrated stereo system, the disparity $d(x,y)$ links one point $I_r(x,y)$ in the reference image with the corresponding $I_t(x',y)$ point in the target, with $x'=x-d(x,y)$. 
						Once the two fictitious images have been generated, a stereo matcher processes them and produces a disparity map, that is then triangulated back into a densified depth map.
					</p>


					<p style="text-align: justify;">
						For projection: we manage real-valued disparities and occlusions, respectively by i) applying <i>weighted splatting</i> in the target image and ii) reprojecting the foreground pattern on occluded regions, as in <a href="https://vppstereo.github.io">Active Stereo Without Pattern Projector</a>.
						Independently of the pattern choice, discussed next, the process outlined is feasible only for a subset of the image points, and we set other points to a constant color (e.g., black in all our experiments). Therefore, from a different point of view, each fictitious camera gathers sparse content coherent with the 3D structure of the scene only where a fictitious virtual pattern projector sends its rays. 
						Regarding the virtually projected patterns, we outline the two following strategies.
					</p>
					
					<ol type="i">
						<li>
							<strong>RGB Projection.</strong> We project onto the two fictitious images the same content $I(x,y)$ from the real camera, for each pixel with an available disparity value:
							$$
							\begin{split}
								I_r(x,y) \leftarrow I(x,y), &\hspace{1cm}
								I_t(x',y) \leftarrow I(x,y), &\hspace{1cm} x'={} &x-d(x,y)
							\end{split} 
							$$
						</li>
						
						<li>
							<strong>Random Pattern Projection.</strong> Instead of warping the image content, we project more <i>matching-friendly</i> patterns. Following <a href="https://vppstereo.github.io">Active Stereo Without Pattern Projector</a>, we project coherently distinctive patterns onto the two fictitious cameras:
							$$
							\begin{split}
								I_r(x,y) \leftarrow \mathcal{P}, &\hspace{1cm}
								I_t(x',y) \leftarrow \mathcal{P}, &\hspace{1cm}
								x'={} &x-d(x,y)
							\end{split} 
							$$
							where operator $\mathcal{P}$ generates a random point-wise pattern applied coherently to both images. 
						</li>
					</ol>

					<p style="text-align: justify;">
						On the one hand, compared to RGB Projection the random patterns are inherently less ambiguous by construction, for instance, in regions featuring a uniform texture. 
					</p>
					<p style="text-align: justify;">
						On the other hand, the sparse patterning prevents a complete awareness of the whole scene content for both strategies. However, such cue can be partially recovered from the RGB image if the stereo matcher can exploit image context.
					</p>

					<center>
						<img class="round" width="1000px" src="./assets/rgb-vs-rnd.png" />
					</center>
					<table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
						<tr>
							<td>
								<p style="text-align: justify;">
									<strong>Qualitative view on uniform areas.</strong> 
									RGB projection keeps uniform regions as such, while random projection makes them distinctive.
								</p>
							</td>
						</tr>
					</table>

					<p style="text-align: justify;">
						We extend the strategy outlined so far to i) increase pattern density according to the RGB content and ii) handle issues regarding the horizontal field of view in the stereo system. 
					</p>

					<ol type="i">
						<li>
							<strong>Adaptive Patch-based Pattern Projection.</strong> The basic point-wise patterning strategy can be expanded to enhance pattern density at nearby points by assuming the same disparity value locally. However, this approach may result in the degradation of fine details. To address this issue, we leverage the RGB image $I$ to extract dense and meaningful scene cues. We propose a heuristic inspired by the bilateral filter to adapt the shape of the patch and handle overlapping patches.
							For a fixed-size patch $\mathcal{N}(x,y)$ centered on an available disparity point $(x,y)$, we estimate the <em>consistency</em> of each nearby point $(x+x_{w},y+y_{w})$ within $\mathcal{N}(x,y)$ with the central point as:

							$$ W_{(x+x_{w},y+y_{w})} = e^{-\left(\frac{\left(x_{w}\right)^{2}+\left(y_{w}\right)^{2}}{2\sigma_{xy}^{2}}+\frac{\left(I\left(x+x_{w},y+y_{w}\right)-I\left(x,y\right)\right)^{2}}{2\sigma_{i}^{2}}\right)} $$
							
							Additionally, we update the upper threshold similarity scores in a data structure of the image size initialized to zero for each available sparse disparity point. Hence, we can project the random pattern with the highest score for overlapping patches. 
						</li>
						<li>
							<strong>Image Padding.</strong> As for any stereo setup, our made of two virtual cameras inherits a well-known issue: the cameras do not frame a completely overlapping portion of the scene.
							Specifically, the left border of the reference image will not be visible in the target image.
							However, since we have complete control over image generation, we can easily eliminate this issue by extending the field of view of our fictitious cameras on the left side, by applying image padding to account for the largest warped point out of the image $w_\text{out}$.
							Accordingly, we can project virtual patterns that otherwise would pop out the left image border. 
							Ultimately, the only trick needed is left cropping the output dense disparity map.
						</li>
					</ol>
				</td>
			</tr>
		</center>
	</table>

	<br>
	<br>
	<hr>

	<center>
		<h1>Qualitative Results</h1>
	</center>

	<br>


	<table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
		<center>
			<tr>
				<td>
					<p style="text-align: justify;">
						We conclude by showing some qualitative examples to confirm the benefits of our method in terms of cross domain generalization.
					</p>
					
					<table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
						<tr>
							<td><img src="assets/indoor2outdoor.png" width="995px" /></td>
						</tr>
						<tr>
					</table>
					<table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
						<tr>
							<td>
								<p style="text-align: justify;">
									<strong>From indoor to outdoor generalization capabilities.</strong> All networks are pretrained on SceneFlow and fine-tuned on NYU. 
									NLPSN and CompletionFormer seem unable to generalize to outdoor data, while SpAgNet can produce some meaningful depth maps, yet far from being accurate. Finally, VPP4DC can improve the results even further thanks to the pre-training process.
								</p>
							</td>
						</tr>
					</table>

					<table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
						<tr>
							<td><img src="assets/outdoor2indoor.png" width="995px" /></td>
						</tr>
						<tr>
					</table>
					<table align=center width="1000px" style="font-family: 'Noto Sans', sans-serif; font-size: 1.1em ">
						<tr>
							<td>
								<p style="text-align: justify;">
									<strong>From outdoor to indoor generalization capabilities.</strong>
									We consider the case complementary to the previous one -- i.e., with models pre-trained on SceneFlow and trained on KITTIDC then tested on NYU. NLSPN, CompletionFormer and SpAgNet can predict a depth map that is reasonable to some extent. Our approach instead predicts very accurate results on regions covered by depth hints, yet fails where these are absent.
								</p>
							</td>
						</tr>
					</table>
				</td>
			</tr>
		</center>
	</table>

	<br>
	<hr>

	<div class="container is-max-desktop content">
		<center>
			<h2 class="bibtex" style="font-size:32px; font-weight:400;">BibTeX</h2>
		</center>
		<pre style="background-color: #f5f5f5; color: #4a4a4a; font-size: 1.5em; overflow-x: auto;">
			<code >
@inproceedings{bartolomei2024revisiting,
  title={Revisiting depth completion from a stereo matching perspective for cross-domain generalization},
  author={Bartolomei, Luca and Poggi, Matteo and Conti, Andrea and Tosi, Fabio and , Stefano},
  booktitle={2024 International Conference on 3D Vision (3DV)},
  pages={1360--1370},
  year={2024},
  organization={IEEE}
}
			</code>
		</pre>
	</div>


	<br>
	<br>
	<hr>
</body>

</html>
